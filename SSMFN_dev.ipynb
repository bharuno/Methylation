{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Methylation_torch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7EzA8jIqqt3"
      },
      "source": [
        "#Neural Network Model - lib Pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "#from better_lstm import LSTM\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn import metrics\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFfDLg-yfU0Z"
      },
      "source": [
        "#pip install git+https://github.com/keitakurita/Better_LSTM_PyTorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGKZ5E1LFv4y"
      },
      "source": [
        "#Read Data\n",
        "\n",
        "#read txt data\n",
        "negative_training_set1_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/negative_training_set1(1038).txt', sep = \"\\t\", header = None)\n",
        "negative_training_set2_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/negative_training_set2(1038).txt', sep = \"\\t\", header = None)\n",
        "negative_training_set3_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/negative_training_set3(1038).txt', sep = \"\\t\", header = None)\n",
        "negative_training_set4_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/negative_training_set4(1038).txt', sep = \"\\t\", header = None)\n",
        "negative_training_set5_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/negative_training_set5(1038).txt', sep = \"\\t\", header = None)\n",
        "positive_training_set_txt  = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/positive_training_set(1038).txt',  sep = \"\\t\", header = None)\n",
        "\n",
        "negative_test_set_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/negative_test_set(260).txt', sep = \"\\t\", header = None)\n",
        "positive_test_set_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/positive_test_set(260).txt', sep = \"\\t\", header = None)\n",
        "\n",
        "independent_negative_set_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/independent_negative_set(3033).txt', sep = \"\\t\", header = None)\n",
        "independent_positive_set_txt = pd.read_csv('https://raw.githubusercontent.com/bharuno/Methylation/main/independent_positive_set(1131).txt', sep = \"\\t\", header = None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2ZnGD2MrNM7"
      },
      "source": [
        "#Data Preprocessing - Convert to Dataframe\n",
        "\n",
        "#def for convert txt data to dataframe\n",
        "def preprocess_data(data):\n",
        "    data1 = data[data.isnull().any(axis=1)].reset_index()\n",
        "    data2 = data.dropna().reset_index()\n",
        "    data3 = pd.concat([data1, data2], axis=1, sort=False, ignore_index=True)\n",
        "    data3.drop(columns=[0,2,3], inplace=True)\n",
        "    data3.rename(index=str, columns={1: \"name\", 4: \"position\", 5: \"sequence\"}, inplace = True)\n",
        "    return data3\n",
        "\n",
        "#applying def\n",
        "negative_training_set1 = preprocess_data(negative_training_set1_txt)\n",
        "negative_training_set2 = preprocess_data(negative_training_set2_txt)\n",
        "negative_training_set3 = preprocess_data(negative_training_set3_txt)\n",
        "negative_training_set4 = preprocess_data(negative_training_set4_txt)\n",
        "negative_training_set5 = preprocess_data(negative_training_set5_txt)\n",
        "positive_training_set = preprocess_data(positive_training_set_txt)\n",
        "\n",
        "negative_test_set = preprocess_data(negative_test_set_txt)\n",
        "positive_test_set = preprocess_data(positive_test_set_txt)\n",
        "\n",
        "independent_negative_set = preprocess_data(independent_negative_set_txt)\n",
        "independent_positive_set = preprocess_data(independent_positive_set_txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "mLvzZMRc4HQo",
        "outputId": "e8452c31-4c9d-4c32-8273-5ae233c9abbe"
      },
      "source": [
        "negative_training_set1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>position</th>\n",
              "      <th>sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&gt;Q8VBY2</td>\n",
              "      <td>234</td>\n",
              "      <td>NLYLVFDLLRKGPVMEVPC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&gt;Tb927.11.12390</td>\n",
              "      <td>641</td>\n",
              "      <td>ICMMLLEGLRQSASFGFDT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&gt;Tb927.5.3710</td>\n",
              "      <td>438</td>\n",
              "      <td>MLCIIYMVARPYLQMHPTR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&gt;Tb927.11.560</td>\n",
              "      <td>134</td>\n",
              "      <td>GDASILFFTRITAWLRLTY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&gt;Tb11.01.1560</td>\n",
              "      <td>29</td>\n",
              "      <td>TCPKGCNSYRHVSTGSDEC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>&gt;Q9NR22</td>\n",
              "      <td>204</td>\n",
              "      <td>SMLNTVIFARDKWLKPGGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1034</th>\n",
              "      <td>&gt;Q6KCD5</td>\n",
              "      <td>1195</td>\n",
              "      <td>EMMDSSTFKRFTASIENIL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>&gt;Q6KCD5</td>\n",
              "      <td>2715</td>\n",
              "      <td>AICCPKYKDRPQIARVVQR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>&gt;Tb927.8.780</td>\n",
              "      <td>542</td>\n",
              "      <td>EKGQVVKQLRESERQLEMT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>&gt;Q80U44</td>\n",
              "      <td>196</td>\n",
              "      <td>NNINAGIKNRDISIKELGV</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1038 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 name position             sequence\n",
              "0             >Q8VBY2      234  NLYLVFDLLRKGPVMEVPC\n",
              "1     >Tb927.11.12390      641  ICMMLLEGLRQSASFGFDT\n",
              "2       >Tb927.5.3710      438  MLCIIYMVARPYLQMHPTR\n",
              "3       >Tb927.11.560      134  GDASILFFTRITAWLRLTY\n",
              "4       >Tb11.01.1560       29  TCPKGCNSYRHVSTGSDEC\n",
              "...               ...      ...                  ...\n",
              "1033          >Q9NR22      204  SMLNTVIFARDKWLKPGGL\n",
              "1034          >Q6KCD5     1195  EMMDSSTFKRFTASIENIL\n",
              "1035          >Q6KCD5     2715  AICCPKYKDRPQIARVVQR\n",
              "1036     >Tb927.8.780      542  EKGQVVKQLRESERQLEMT\n",
              "1037          >Q80U44      196  NNINAGIKNRDISIKELGV\n",
              "\n",
              "[1038 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkjum7PVrpZt"
      },
      "source": [
        "#Data Preprocessing - Sequence Extraction and Labeling\n",
        "\n",
        "#take the sequence\n",
        "negative_seq1 = np.array([ list(word) for word in negative_training_set1.sequence.values])\n",
        "negative_seq2 = np.array([ list(word) for word in negative_training_set2.sequence.values])\n",
        "negative_seq3 = np.array([ list(word) for word in negative_training_set3.sequence.values])\n",
        "negative_seq4 = np.array([ list(word) for word in negative_training_set4.sequence.values])\n",
        "negative_seq5 = np.array([ list(word) for word in negative_training_set5.sequence.values])\n",
        "negative_seq = np.concatenate((negative_seq1,\n",
        "                               negative_seq2,\n",
        "                               negative_seq3,\n",
        "                               negative_seq4,\n",
        "                               negative_seq5), axis=0, out=None)\n",
        "positive_seq = np.array([ list(word) for word in positive_training_set.sequence.values])\n",
        "\n",
        "negative_seq_val = np.array([ list(word) for word in independent_negative_set.sequence.values])\n",
        "positive_seq_val = np.array([ list(word) for word in independent_positive_set.sequence.values])\n",
        "\n",
        "negative_seq_test = np.array([ list(word) for word in negative_test_set.sequence.values])\n",
        "positive_seq_test = np.array([ list(word) for word in positive_test_set.sequence.values])\n",
        "\n",
        "#create label - training data\n",
        "negative_lab1 = np.zeros((negative_seq1.shape[0],), dtype=int)\n",
        "negative_lab = np.zeros((negative_seq.shape[0],), dtype=int)\n",
        "positive_lab = np.ones((positive_seq.shape[0],), dtype=int)\n",
        "negative_lab_val = np.zeros((negative_seq_val.shape[0],), dtype=int)\n",
        "positive_lab_val = np.ones((positive_seq_val.shape[0],), dtype=int)\n",
        "negative_lab_test = np.zeros((negative_seq_test.shape[0],), dtype=int)\n",
        "positive_lab_test = np.ones((positive_seq_test.shape[0],), dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnJ5GrOs3-Mu",
        "outputId": "06f9b41b-fd9d-4816-abcd-87785e312ee7"
      },
      "source": [
        "dataset_Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z-8lcP3si5d"
      },
      "source": [
        "#Data Preprocessing - Decide dataset characteristics, Merging and Tokenizing \n",
        "\n",
        "#Parameters\n",
        "\n",
        "balanced_data = False\n",
        "token = True\n",
        "window = 19 #3 to 19 odd number\n",
        "\n",
        "if (balanced_data):\n",
        "  df_train_pos = positive_seq\n",
        "  df_train_neg = negative_seq1\n",
        "  df_lab_pos = positive_lab\n",
        "  df_lab_neg = negative_lab1\n",
        "else:\n",
        "  df_train_pos = positive_seq\n",
        "  df_train_neg = negative_seq\n",
        "  df_lab_pos = positive_lab\n",
        "  df_lab_neg = negative_lab\n",
        "\n",
        "start_w = 9-int(window/2)\n",
        "end_w = 9+int(window/2)+1\n",
        "\n",
        "#merge\n",
        "dataset_X = np.concatenate((df_train_pos, df_train_neg), axis=0, out=None)\n",
        "dataset_Y = np.concatenate((df_lab_pos, df_lab_neg), axis=0, out=None)\n",
        "dataset_X_val = np.concatenate((positive_seq_val, negative_seq_val), axis=0, out=None)\n",
        "dataset_Y_val = np.concatenate((positive_lab_val, negative_lab_val), axis=0, out=None)\n",
        "dataset_X_test = np.concatenate((positive_seq_test, negative_seq_test), axis=0, out=None)\n",
        "dataset_Y_test = np.concatenate((positive_lab_test, negative_lab_test), axis=0, out=None)\n",
        "\n",
        "negative_seq_val_eq = negative_seq_val[1:len(positive_seq_val)+1]\n",
        "negative_lab_val_eq = negative_lab_val[1:len(positive_lab_val)+1]\n",
        "dataset_X_val_eq = np.concatenate((positive_seq_val, negative_seq_val_eq), axis=0, out=None)\n",
        "dataset_Y_val_eq = np.concatenate((positive_lab_val, negative_lab_val_eq), axis=0, out=None)\n",
        "\n",
        "#Tokenizing, Unique character got its own number - training\n",
        "asam = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V','X']\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(asam)\n",
        "\n",
        "dataset_X_token = []\n",
        "for i in range(len(dataset_X)):\n",
        "    temp = tokenizer.texts_to_sequences(dataset_X[i])\n",
        "    dataset_X_token = np.append(dataset_X_token, temp)\n",
        "\n",
        "dataset_X_token = dataset_X_token-1\n",
        "dataset_X_token = dataset_X_token.reshape(len(dataset_X),19)\n",
        "dataset_X_token =  dataset_X_token[:, range(start_w, end_w)]\n",
        "\n",
        "#Tokenizing, Unique character got its own number - validation\n",
        "dataset_X_token_val = []\n",
        "for i in range(len(dataset_X_val)):\n",
        "    temp = tokenizer.texts_to_sequences(dataset_X_val[i])\n",
        "    dataset_X_token_val = np.append(dataset_X_token_val, temp)\n",
        "\n",
        "dataset_X_token_val = dataset_X_token_val-1\n",
        "dataset_X_token_val = dataset_X_token_val.reshape(len(dataset_X_val),19)\n",
        "dataset_X_token_val =  dataset_X_token_val[:, range(start_w, end_w)]\n",
        "\n",
        "#Tokenizing, Unique character got its own number - validation equal\n",
        "dataset_X_token_val_eq = []\n",
        "for i in range(len(dataset_X_val_eq)):\n",
        "    temp = tokenizer.texts_to_sequences(dataset_X_val_eq[i])\n",
        "    dataset_X_token_val_eq = np.append(dataset_X_token_val_eq, temp)\n",
        "\n",
        "dataset_X_token_val_eq = dataset_X_token_val_eq-1\n",
        "dataset_X_token_val_eq = dataset_X_token_val_eq.reshape(len(dataset_X_val_eq),19)\n",
        "dataset_X_token_val_eq =  dataset_X_token_val_eq[:, range(start_w, end_w)]\n",
        "\n",
        "#Tokenizing, Unique character got its own number - testing\n",
        "dataset_X_token_test = []\n",
        "for i in range(len(dataset_X_test)):\n",
        "    temp = tokenizer.texts_to_sequences(dataset_X_test[i])\n",
        "    dataset_X_token_test = np.append(dataset_X_token_test, temp)\n",
        "\n",
        "dataset_X_token_test = dataset_X_token_test-1\n",
        "dataset_X_token_test = dataset_X_token_test.reshape(len(dataset_X_test),19)\n",
        "dataset_X_token_test =  dataset_X_token_test[:, range(start_w, end_w)]\n",
        "\n",
        "#for if con ensamble\n",
        "modelCNN_pred_val_imbalance = []\n",
        "modelLSTM_pred_val_imbalance = [] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYRzE9YDLqE0",
        "outputId": "86cea0b0-9f54-436d-b046-42cbb2454836"
      },
      "source": [
        "dataset_X_token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2., 15.,  7., ..., 18.,  9., 13.],\n",
              "       [15.,  0.,  7., ...,  1., 17.,  5.],\n",
              "       [15., 10.,  5., ...,  1., 11., 16.],\n",
              "       ...,\n",
              "       [ 3., 15., 16., ..., 11.,  1., 12.],\n",
              "       [10., 13., 10., ...,  9.,  3., 14.],\n",
              "       [ 9.,  0., 10., ..., 19.,  2.,  2.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d0th39yz3qa"
      },
      "source": [
        "#Creating the Neural Network\n",
        "\n",
        "#logistic model\n",
        "class ModelLog(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(ModelLog, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "        \n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "# NN sample\n",
        "class NeuralNet1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(NeuralNet1, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # sigmoid at the end\n",
        "        y_pred = torch.sigmoid(out)\n",
        "        return y_pred\n",
        "\n",
        "# Our method\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self, embedding_vec):\n",
        "        super(Model1, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(asam), embedding_vec) \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.batchNorm1 = nn.BatchNorm1d(embedding_vec*n_features)\n",
        "        self.linear1 = nn.Linear(embedding_vec*n_features, embedding_vec*n_features)\n",
        "        #self.batchNorm2 = nn.BatchNorm1d(embedding_vec*n_features)\n",
        "        self.output = nn.Linear(embedding_vec*n_features, 2)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.flatten(out)\n",
        "        out = self.batchNorm1(out)\n",
        "        out = self.linear1(out)        \n",
        "        #out = self.batchNorm2(out)\n",
        "        y_pred = self.output(out)\n",
        "        # sigmoid at the end\n",
        "        #y_pred = torch.sigmoid(out)\n",
        "        return y_pred\n",
        "\n",
        "# Our method\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, embedding_vec):\n",
        "        super(Model2, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(asam), embedding_vec) \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.batchNorm1 = nn.BatchNorm1d(embedding_vec*n_features)\n",
        "        self.linear1 = nn.Linear(embedding_vec*n_features, embedding_vec*n_features)\n",
        "        #self.batchNorm2 = nn.BatchNorm1d(embedding_vec*n_features)\n",
        "        self.output = nn.Linear(embedding_vec*n_features, 1)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.flatten(out)\n",
        "        out = self.batchNorm1(out)\n",
        "        out = self.linear1(out)        \n",
        "        #out = self.batchNorm2(out)\n",
        "        out = self.output(out)\n",
        "        # sigmoid at the end\n",
        "        y_pred = torch.sigmoid(out)\n",
        "        return y_pred\n",
        "\n",
        "# CNN\n",
        "class ModelCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(asam), 21) \n",
        "        self.conv1 = nn.Conv2d(1, 64, (9,3)) \n",
        "        self.conv2 = nn.Conv2d(64, 128, (9,3), padding = (4,1))\n",
        "        self.drop1 = nn.Dropout(p=0.6)\n",
        "        self.drop2 = nn.Dropout(p=0.6)\n",
        "        self.drop3 = nn.Dropout(p=0.5)\n",
        "        self.drop4 = nn.Dropout(p=0.5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear1 = nn.Linear(5760, 768) \n",
        "        self.linear2 = nn.Linear(768, 256) \n",
        "        self.linear3 = nn.Linear(256, 2) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = out.view(out.shape[0], 1, out.shape[1], out.shape[2]) #channel, add dim, seq, emd vec\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.drop1(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.drop2(out)\n",
        "        \n",
        "        out = self.pool(out)\n",
        "        out = self.flatten(out)\n",
        "        \n",
        "        out = self.linear1(out)\n",
        "        out = self.drop3(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.drop4(out)\n",
        "\n",
        "        y_pred = self.linear3(out)\n",
        "        return y_pred\n",
        "\n",
        "# LSTM\n",
        "class ModelLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(asam), 21) \n",
        "\n",
        "        self.lstm1 = nn.LSTM(21, 128, 1)\n",
        "        self.lstm2 = nn.LSTM(128, 64, 1)\n",
        "        self.drop1 = nn.Dropout(p=0.5)\n",
        "        #self.betterlstm = LSTM(128, 64, dropoutw=0.2)\n",
        "        self.linear1 = nn.Linear(64, 32) \n",
        "        self.linear2 = nn.Linear(32, 2) \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = out.permute(1,0,2)\n",
        "        out = self.lstm1(out)\n",
        "        out = self.lstm2(out[0])\n",
        "        out = self.drop1(out[0])\n",
        "        #out = self.betterlstm(out[0])\n",
        "        #pdb.set_trace()\n",
        "        #out = out[0]\n",
        "        #out = self.linear1(out[0][out[0].shape[0]-1])\n",
        "        out = self.linear1(out[out.shape[0]-1])\n",
        "        y_pred = self.linear2(out)\n",
        "        return y_pred\n",
        "\n",
        "# Neo\n",
        "class ModelNeo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelNeo, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(asam), 21)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(21, 64, 1)\n",
        "        self.lstm2 = nn.LSTM(64, 64, 1)\n",
        "        self.lstm3 = nn.LSTM(64, 64, 1)\n",
        "        self.lstm4 = nn.LSTM(64, 64, 1)\n",
        "        self.batchnormL1 = nn.BatchNorm1d(64)\n",
        "        self.batchnormL2 = nn.BatchNorm1d(64)\n",
        "        self.batchnormL3 = nn.BatchNorm1d(64)\n",
        "        self.batchnormL4 = nn.BatchNorm1d(64)\n",
        "        self.dropL1 = nn.Dropout(p=0.5)\n",
        "        self.dropL2 = nn.Dropout(p=0.5)\n",
        "        self.dropL3 = nn.Dropout(p=0.5)\n",
        "        self.dropL4 = nn.Dropout(p=0.5)\n",
        "        self.linearL1 = nn.Linear(64, 32) \n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, (3,21), padding = (1,0)) \n",
        "        self.conv2 = nn.Conv2d(64, 64, (3,1), padding = (1,0)) \n",
        "        self.conv3 = nn.Conv2d(64, 64, (3,1), padding = (1,0)) \n",
        "        self.conv4 = nn.Conv2d(64, 64, (3,1), padding = (1,0))\n",
        "        self.batchnormC1 = nn.BatchNorm2d(64)\n",
        "        self.batchnormC2 = nn.BatchNorm2d(64)\n",
        "        self.batchnormC3 = nn.BatchNorm2d(64)\n",
        "        self.batchnormC4 = nn.BatchNorm2d(64)\n",
        "        self.dropC1 = nn.Dropout(p=0.5)\n",
        "        self.dropC2 = nn.Dropout(p=0.5)\n",
        "        self.dropC3 = nn.Dropout(p=0.5)\n",
        "        self.dropC4 = nn.Dropout(p=0.5)\n",
        "        #self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linearC1 = nn.Linear(1216, 32) \n",
        "        \n",
        "        self.linear2 = nn.Linear(32, 2) \n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        lstm = emb.permute(1,0,2)\n",
        "        \n",
        "        lstm = self.lstm1(lstm)\n",
        "        # lstm = lstm[0].permute(1,2,0)\n",
        "        # lstm = self.batchnormL1(lstm)\n",
        "        # lstm = lstm.permute(2,0,1)\n",
        "        lstm1 = self.dropL1(lstm[0])\n",
        "\n",
        "        lstm = self.lstm2(lstm1)\n",
        "        # lstm = lstm[0].permute(1,2,0)\n",
        "        # lstm = self.batchnormL2(lstm)\n",
        "        # lstm = lstm.permute(2,0,1)\n",
        "        lstm = self.dropL2(lstm[0])\n",
        "        #lstm2 = lstm + lstm1\n",
        "\n",
        "        # lstm = self.lstm3(lstm2)\n",
        "        # lstm = lstm[0].permute(1,2,0)\n",
        "        # lstm = self.batchnormL3(lstm)\n",
        "        # lstm = lstm.permute(2,0,1)\n",
        "        # lstm = self.dropL3(lstm)\n",
        "        # lstm3 = lstm + lstm2\n",
        "\n",
        "        # lstm = self.lstm4(lstm3)\n",
        "        # lstm = lstm[0].permute(1,2,0)\n",
        "        # lstm = self.batchnormL4(lstm)\n",
        "        # lstm = lstm.permute(2,0,1)\n",
        "        # lstm = self.dropL4(lstm)\n",
        "        # lstm4 = lstm + lstm3\n",
        "        #lstm = lstm2\n",
        "        \n",
        "        lstm = self.linearL1(lstm[lstm.shape[0]-1])\n",
        "        \n",
        "        cnn = emb.view(emb.shape[0], 1, emb.shape[1], emb.shape[2]) #channel, add dim, seq, emd vec\n",
        "        \n",
        "        cnn = self.conv1(cnn)\n",
        "        cnn = F.relu(cnn)\n",
        "        cnn = self.batchnormC1(cnn)\n",
        "        cnn1 = self.dropC1(cnn)\n",
        "\n",
        "        cnn = self.conv2(cnn1)\n",
        "        cnn = F.relu(cnn)\n",
        "        cnn = self.batchnormC2(cnn)\n",
        "        cnn = self.dropC2(cnn)\n",
        "        cnn2 = cnn + cnn1\n",
        "\n",
        "        cnn = self.conv3(cnn2)\n",
        "        cnn = F.relu(cnn)\n",
        "        cnn = self.batchnormC3(cnn)\n",
        "        cnn = self.dropC3(cnn)\n",
        "        cnn3 = cnn + cnn2\n",
        "\n",
        "        cnn = self.conv4(cnn3)\n",
        "        cnn = F.relu(cnn)\n",
        "        cnn = self.batchnormC4(cnn)\n",
        "        cnn = self.dropC4(cnn)\n",
        "        cnn4 = cnn + cnn3\n",
        "        \n",
        "\n",
        "        #cnn = self.pool(cnn)\n",
        "        cnn = self.flatten(cnn4)\n",
        "        cnn = self.linearC1(cnn)\n",
        "\n",
        "        out = cnn\n",
        "        y_pred = self.linear2(out)\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpBHWuGu2t5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fea35ed-f5a8-4125-c719-d8392823ff4f"
      },
      "source": [
        "#Training the data\n",
        "\n",
        "#model = ModelLog(n_features)\n",
        "#model = NeuralNet1(input_size=n_features, hidden_size=5)\n",
        "\n",
        "#Choose model\n",
        "model_which = 'modelNeo' #'model1' 'modelCNN' 'modelLSTM'\n",
        "\n",
        "#adjust input output type\n",
        "if (model_which == 'model1'):\n",
        "  inputType = np.int\n",
        "  output_onehot = False\n",
        "  output_type = np.int\n",
        "\n",
        "elif (model_which == 'model2'):\n",
        "  inputType = np.int\n",
        "  output_onehot = False\n",
        "  output_type = np.float32\n",
        "  \n",
        "elif (model_which == 'modelCNN'):\n",
        "  inputType = np.int\n",
        "  output_onehot = False\n",
        "  output_type = np.int  \n",
        "\n",
        "elif (model_which == 'modelLSTM'):\n",
        "  inputType = np.int\n",
        "  output_onehot = False\n",
        "  output_type = np.int\n",
        "  \n",
        "elif (model_which == 'modelNeo'):\n",
        "  inputType = np.int\n",
        "  output_onehot = False\n",
        "  output_type = np.int\n",
        "\n",
        "else:\n",
        "  raise Exception(\"Sorry, no model named that\")\n",
        "\n",
        "\n",
        "#Shuffle Dataset\n",
        "if (token):\n",
        "  X_train, y_train = shuffle(dataset_X_token, dataset_Y, random_state=13)\n",
        "  X_val, y_val = shuffle(dataset_X_token_val, dataset_Y_val, random_state=13)\n",
        "  X_val_eq, y_val_eq = shuffle(dataset_X_token_val_eq, dataset_Y_val_eq, random_state=13)\n",
        "  X_test, y_test = shuffle(dataset_X_token_test, dataset_Y_test, random_state=13)\n",
        "else:\n",
        "  X_train, y_train = shuffle(dataset_X, dataset_Y, random_state=13)\n",
        "  X_val, y_val = shuffle(dataset_X_val, dataset_Y_val, random_state=13)\n",
        "  X_val_eq, y_val_eq = shuffle(dataset_X_val_eq, dataset_Y_val_eq, random_state=13)\n",
        "  X_test, y_test = shuffle(dataset_X_test, dataset_Y_test, random_state=13)\n",
        "\n",
        "#convert X vars to torch\n",
        "X_train_torch = torch.from_numpy(X_train.astype(inputType)).cuda()\n",
        "X_val_torch = torch.from_numpy(X_val.astype(inputType)).cuda()\n",
        "X_val_eq_torch = torch.from_numpy(X_val_eq.astype(inputType)).cuda()\n",
        "X_test_torch = torch.from_numpy(X_test.astype(inputType)).cuda()\n",
        "\n",
        "#adjust Y vars size\n",
        "output_size = 1\n",
        "if (output_onehot):\n",
        "  temp1 = np.expand_dims(y_train, axis=0).reshape(y_train.shape[0],1)\n",
        "  temp1 = temp1.reshape(y_train.shape[0],1)\n",
        "  temp2 = np.expand_dims(y_train, axis=0)*-1+1\n",
        "  temp2 = temp2.reshape(y_train.shape[0],1)\n",
        "  y_train = np.concatenate((temp1, temp2), axis=1, out=None)\n",
        "\n",
        "  temp1 = np.expand_dims(y_val, axis=0).reshape(y_val.shape[0],1)\n",
        "  temp1 = temp1.reshape(y_val.shape[0],1)\n",
        "  temp2 = np.expand_dims(y_val, axis=0)*-1+1\n",
        "  temp2 = temp2.reshape(y_val.shape[0],1)\n",
        "  y_val = np.concatenate((temp1, temp2), axis=1, out=None)\n",
        "  \n",
        "  temp1 = np.expand_dims(y_val_eq, axis=0).reshape(y_val_eq.shape[0],1)\n",
        "  temp1 = temp1.reshape(y_val_eq.shape[0],1)\n",
        "  temp2 = np.expand_dims(y_val_eq, axis=0)*-1+1\n",
        "  temp2 = temp2.reshape(y_val_eq.shape[0],1)\n",
        "  y_val_eq = np.concatenate((temp1, temp2), axis=1, out=None)\n",
        "\n",
        "  temp1 = np.expand_dims(y_test, axis=0).reshape(y_test.shape[0],1)\n",
        "  temp1 = temp1.reshape(y_test.shape[0],1)\n",
        "  temp2 = np.expand_dims(y_test, axis=0)*-1+1\n",
        "  temp2 = temp2.reshape(y_test.shape[0],1)\n",
        "  y_test = np.concatenate((temp1, temp2), axis=1, out=None)\n",
        "\n",
        "  output_size = 2\n",
        "\n",
        "#convert Y vars to torch\n",
        "y_train_torch = torch.from_numpy(y_train.astype(output_type)).cuda()\n",
        "y_val_torch = torch.from_numpy(y_val.astype(output_type)).cuda()\n",
        "y_val_eq_torch = torch.from_numpy(y_val_eq.astype(output_type)).cuda()\n",
        "y_test_torch = torch.from_numpy(y_test.astype(output_type)).cuda()\n",
        "\n",
        "#reshape for model 2(sigmoid out)\n",
        "if (model_which == 'model2'):\n",
        "  y_train_torch = y_train_torch.view(y_train_torch.shape[0], output_size).cuda()\n",
        "  y_val_torch = y_val_torch.view(y_val_torch.shape[0], output_size).cuda()\n",
        "  y_val_eq_torch = y_val_eq_torch.view(y_val_eq_torch.shape[0], output_size).cuda()\n",
        "  y_test_torch = y_test_torch.view(y_test_torch.shape[0], output_size).cuda()\n",
        "\n",
        "n_samples, n_features = X_train_torch.shape\n",
        "\n",
        "#model hyper parameter\n",
        "if (model_which == 'model1'):\n",
        "  embedding_vector = 3\n",
        "  model = Model1(embedding_vector).cuda()\n",
        "  num_epochs = 10000\n",
        "  learning_rate = 0.001\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
        "\n",
        "elif (model_which == 'model2'):\n",
        "  embedding_vector = 3\n",
        "  model = Model2(embedding_vector).cuda()\n",
        "  num_epochs = 1000\n",
        "  learning_rate = 0.001\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
        "  \n",
        "elif (model_which == 'modelCNN'):\n",
        "  model = ModelCNN().cuda()\n",
        "  num_epochs = 500\n",
        "  learning_rate = 0.0001\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "elif (model_which == 'modelLSTM'):\n",
        "  model = ModelLSTM().cuda()\n",
        "  num_epochs = 500\n",
        "  learning_rate = 0.001\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "elif (model_which == 'modelNeo'):\n",
        "  model = ModelNeo().cuda()\n",
        "  num_epochs = 500\n",
        "  learning_rate = 0.001\n",
        "  weight_loss = torch.tensor([0.17, 0.83]).cuda()\n",
        "  criterion = nn.CrossEntropyLoss()#weight=weight_loss\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "\n",
        "\n",
        "#Validation\n",
        "print('train X shape: ', X_train_torch.shape)\n",
        "print('train Y shape: ', y_train_torch.shape)\n",
        "print()\n",
        "print('validation X shape: ', X_val_torch.shape)\n",
        "print('validation Y shape: ', y_val_torch.shape)\n",
        "print()\n",
        "print('test X shape: ', X_test_torch.shape)\n",
        "print('test Y shape: ', y_test_torch.shape)\n",
        "print()\n",
        "print('input sample: ', X_train_torch[0])\n",
        "print()\n",
        "print('Model: ', model_which)\n",
        "if (balanced_data):\n",
        "  print('Training data: balance')\n",
        "else:\n",
        "  print('Training data: imbalance')\n",
        "print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train X shape:  torch.Size([6228, 19])\n",
            "train Y shape:  torch.Size([6228])\n",
            "\n",
            "validation X shape:  torch.Size([4164, 19])\n",
            "validation Y shape:  torch.Size([4164])\n",
            "\n",
            "test X shape:  torch.Size([520, 19])\n",
            "test Y shape:  torch.Size([520])\n",
            "\n",
            "input sample:  tensor([16, 12, 15, 15, 15, 16,  6, 19, 15,  1,  4,  9,  0,  8, 10,  8,  1, 16,\n",
            "         6], device='cuda:0')\n",
            "\n",
            "Model:  modelNeo\n",
            "Training data: imbalance\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2gjGLsRXNcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509f4bce-98ad-49c4-e5f9-9b421f67d659"
      },
      "source": [
        "valid_best_loss = np.inf\n",
        "best_epoch = 0\n",
        "#Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # save best epoch\n",
        "    if (loss.item() < valid_best_loss):\n",
        "      valid_best_loss = loss.item()\n",
        "      best_epoch = epoch\n",
        "      torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    # epoch progress \n",
        "    if (epoch+1) % (num_epochs/10) == 0:\n",
        "        if (model_which == 'model2'): #if use sigmoid\n",
        "          y_pred_cls = y_pred.round()\n",
        "          acc = y_pred_cls.eq(y_train_torch).sum() / float(y_train_torch.shape[0])\n",
        "        else:\n",
        "          acc = torch.max(y_pred, 1)[1].eq(y_train_torch).sum() / float(y_train_torch.shape[0])         \n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          y_predicted_balanced = model(X_val_eq_torch)\n",
        "          if (model_which == 'model2'):#if use sigmoid\n",
        "            y_predicted_cls_balanced = y_predicted_balanced.round()\n",
        "            accv = y_predicted_cls_balanced.eq(y_val_eq_torch).sum() / float(y_val_eq_torch.shape[0])\n",
        "          else:\n",
        "            accv = torch.max(y_predicted_balanced, 1)[1].eq(y_val_eq_torch).sum() / float(y_val_eq_torch.shape[0])\n",
        " \n",
        "          print(f'epoch: {epoch+1}, accuracy = {acc:.4f}, loss = {loss.item():.4f}, val acc = {accv.item():.4f}')\n",
        "\n",
        "print(f'Best Epoch: {best_epoch}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 50, accuracy = 0.8847, loss = 0.3054, val acc = 0.7630\n",
            "epoch: 100, accuracy = 0.8998, loss = 0.2650, val acc = 0.7648\n",
            "epoch: 150, accuracy = 0.9041, loss = 0.2496, val acc = 0.7706\n",
            "epoch: 200, accuracy = 0.9139, loss = 0.2239, val acc = 0.7692\n",
            "epoch: 250, accuracy = 0.9194, loss = 0.2085, val acc = 0.7688\n",
            "epoch: 300, accuracy = 0.9180, loss = 0.2008, val acc = 0.7675\n",
            "epoch: 350, accuracy = 0.9258, loss = 0.1848, val acc = 0.7653\n",
            "epoch: 400, accuracy = 0.9298, loss = 0.1749, val acc = 0.7657\n",
            "epoch: 450, accuracy = 0.9353, loss = 0.1616, val acc = 0.7670\n",
            "epoch: 500, accuracy = 0.9316, loss = 0.1647, val acc = 0.7635\n",
            "Best Epoch: 477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "776hdCGWX7IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7980747-6f50-493a-be7a-62227415bcb8"
      },
      "source": [
        " #validation\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "with torch.no_grad():\n",
        "  y_predicted_imbalanced = model(X_val_torch)\n",
        "  y_predicted_balanced = model(X_val_eq_torch)\n",
        "  acc1 = torch.max(y_predicted_imbalanced, 1)[1].eq(y_val_torch).sum() / float(y_val_torch.shape[0])\n",
        "  acc2 = torch.max(y_predicted_balanced, 1)[1].eq(y_val_eq_torch).sum() / float(y_val_eq_torch.shape[0])\n",
        "  \n",
        "  f1_score_imbalance = f1_score(torch.max(y_predicted_imbalanced, 1)[1].cpu().numpy(), y_val_torch.cpu().numpy(), average='macro')\n",
        "  f1_score_balance = f1_score(torch.max(y_predicted_balanced, 1)[1].cpu().numpy(), y_val_eq_torch.cpu().numpy(), average='macro')\n",
        "\n",
        "  mcc_imbalance = matthews_corrcoef(torch.max(y_predicted_imbalanced, 1)[1].cpu().numpy(), y_val_torch.cpu().numpy())\n",
        "  mcc_balance = matthews_corrcoef(torch.max(y_predicted_balanced, 1)[1].cpu().numpy(), y_val_eq_torch.cpu().numpy())\n",
        "  \n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_val_torch.cpu().numpy(), torch.max(y_predicted_imbalanced, 1)[1].cpu().numpy(), pos_label=1)\n",
        "  auc_imbalance = metrics.auc(fpr, tpr)\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_val_eq_torch.cpu().numpy(), torch.max(y_predicted_balanced, 1)[1].cpu().numpy(), pos_label=1)\n",
        "  auc_balance = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(f'Validation accuracy (imbalance): {acc1.item():.4f}, F1: {f1_score_imbalance.item():.4f}, mcc: {mcc_imbalance.item():.4f}, auc: {auc_imbalance.item():.4f}')\n",
        "print(confusion_matrix(torch.max(y_predicted_imbalanced, 1)[1].cpu().numpy(), y_val_torch.cpu().numpy()))\n",
        "print()\n",
        "print(f'Validation accuracy (balance): {acc2.item():.4f}, F1: {f1_score_balance.item():.4f}, mcc: {mcc_balance.item():.4f}, auc: {auc_imbalance.item():.4f}')\n",
        "print(confusion_matrix(torch.max(y_predicted_balanced, 1)[1].cpu().numpy(), y_val_eq_torch.cpu().numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy (imbalance): 0.8941, F1: 0.8503, mcc: 0.7239, auc: 0.8175\n",
            "[[2988  396]\n",
            " [  45  735]]\n",
            "\n",
            "Validation accuracy (balance): 0.7666, F1: 0.7532, mcc: 0.6023, auc: 0.8175\n",
            "[[1130  527]\n",
            " [   1  604]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L2rWuPs6CJ3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cQFAvWgXWJW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bce-TYdkvx2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613cccf7-80b4-490d-ca24-50096193c0a9"
      },
      "source": [
        "\n",
        "if (model_which == 'modelCNN'):\n",
        "  print('modelCNN')\n",
        "  modelCNN_pred_val_imbalance = nn.Softmax()(y_predicted_imbalanced)\n",
        "  modelCNN_pred_val_balance = nn.Softmax()(y_predicted_balanced)\n",
        "\n",
        "\n",
        "elif (model_which == 'modelLSTM'):\n",
        "  print('modelLSTM')\n",
        "  modelLSTM_pred_val_imbalance = nn.Softmax()(y_predicted_imbalanced)\n",
        "  modelLSTM_pred_val_balance = nn.Softmax()(y_predicted_balanced)\n",
        "else:\n",
        "  print('not running softmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not running softmax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOsEsbEjzuqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aede6aad-6564-4ef0-c310-82bea4f92add"
      },
      "source": [
        "\n",
        "if (modelCNN_pred_val_imbalance!=[]) & (modelLSTM_pred_val_imbalance !=[]):\n",
        " print('both executed')\n",
        " ensamble_pred_imbalance = (modelLSTM_pred_val_imbalance * 0.17) + (modelCNN_pred_val_imbalance * 0.83)\n",
        " ensamble_pred_balance = (modelLSTM_pred_val_balance * 0.17) + (modelCNN_pred_val_balance * 0.83)\n",
        "\n",
        " acc1 = torch.max(ensamble_pred_imbalance, 1)[1].eq(y_val_torch).sum() / float(y_val_torch.shape[0])\n",
        " acc2 = torch.max(ensamble_pred_balance, 1)[1].eq(y_val_eq_torch).sum() / float(y_val_eq_torch.shape[0])\n",
        " f1_score_imbalance = f1_score(torch.max(ensamble_pred_imbalance, 1)[1].cpu().numpy(), y_val_torch.cpu().numpy(), average='macro')\n",
        " f1_score_balance = f1_score(torch.max(ensamble_pred_balance, 1)[1].cpu().numpy(), y_val_eq_torch.cpu().numpy(), average='macro')\n",
        " \n",
        " print()\n",
        " print(f'Validation accuracy (imbalance): {acc1.item():.4f}, F1: {f1_score_imbalance.item():.4f}')\n",
        " print(confusion_matrix(torch.max(ensamble_pred_imbalance, 1)[1].cpu().numpy(), y_val_torch.cpu().numpy()))\n",
        " print(f'Validation accuracy (balance): {acc2.item():.4f}, F1: {f1_score_balance.item():.4f}')\n",
        " print(confusion_matrix(torch.max(ensamble_pred_balance, 1)[1].cpu().numpy(), y_val_eq_torch.cpu().numpy()))\n",
        " \n",
        "else:\n",
        "  print('one of the model or both, have not been executed yet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one of the model or both, have not been executed yet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH4LqA-Th2sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a06c47c-4dbd-417f-c028-57d65d8bd236"
      },
      "source": [
        "#test\n",
        "\n",
        "#print best model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "with torch.no_grad():\n",
        "  y_predicted_test = model(X_test_torch)\n",
        "  acc_test = torch.max(y_predicted_test, 1)[1].eq(y_test_torch).sum() / float(y_test_torch.shape[0])\n",
        "  f1_score_test = f1_score(torch.max(y_predicted_test, 1)[1].cpu().numpy(), y_test_torch.cpu().numpy(), average='macro')\n",
        "  mcc_test = matthews_corrcoef(torch.max(y_predicted_test, 1)[1].cpu().numpy(), y_test_torch.cpu().numpy())\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test_torch.cpu().numpy(), torch.max(y_predicted_test, 1)[1].cpu().numpy(), pos_label=1)\n",
        "  auc_test = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(f'Test accuracy: {acc_test.item():.4f}, F1: {f1_score_test.item():.4f}, mcc: {mcc_test.item():.4f}, auc: {auc_test.item():.4f}')\n",
        "print(confusion_matrix(torch.max(y_predicted_test, 1)[1].cpu().numpy(), y_test_torch.cpu().numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7462, F1: 0.7303, mcc: 0.5628, auc: 0.7462\n",
            "[[257 129]\n",
            " [  3 131]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}